{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_FastText_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dassatabdi24/Humor-level-detection/blob/main/Bert_FastText_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kysbJ1FUXyQE"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cebnAL_GYSbU"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fe6bFytX-Is"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
        "import os\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score\n",
        "import json, time, datetime, pickle, random, copy, torch, zipfile, re, string, math\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAlTHYeKY7E9"
      },
      "source": [
        "CONFIG = {\n",
        "    'pair_max_len': 160,\n",
        "    'single_max_len': 96\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxRVEGOSvD8z"
      },
      "source": [
        "df= pd.read_csv( '/content/gdrive/MyDrive/data/task1/train.csv')\n",
        "print(df)\n",
        "print(df.loc[df['meanGrade'] == 2.0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuWyrfbwww0i"
      },
      "source": [
        "print(df.iloc[103,1])\n",
        "print(df.iloc[103,2])\n",
        "print(df.iloc[103,4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhO0yuvPZAME"
      },
      "source": [
        "class Read_data:\n",
        "\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "\n",
        "    def replace_edit(self, o, e, pair=True):\n",
        "        assert len(o) == len(e)\n",
        "        s_o, s_e = [], []\n",
        "        for i in range(len(o)):\n",
        "            s_o.append(re.sub('<(.+)/>', '\\\\g<1>', o[i]))\n",
        "            s_e.append(re.sub('<.+/>', e[i], o[i]))\n",
        "        if pair:\n",
        "            return list(zip(s_o, s_e))\n",
        "        return s_e\n",
        "\n",
        "    def A_helper(self, df, pair=True):\n",
        "        sents = self.replace_edit(df['original'], df['edit'], pair=pair)\n",
        "        labels = df['meanGrade'].tolist()\n",
        "        return (sents, labels)\n",
        "\n",
        "    def reader(self, pair=True):\n",
        "        return self.A_helper(pd.read_csv(self.path), pair=pair)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6IyD-8cZG-q"
      },
      "source": [
        "\n",
        "def get_pair_ids(sents, tokenizer, max_len=300, pair=True):\n",
        "    input_ids, attention_masks, type_ids = [], [], []\n",
        "    for e in sents:\n",
        "        if pair:\n",
        "            t_e = tokenizer.encode_plus(text=(e[0]), text_pair=(e[1]),\n",
        "            max_length=max_len,\n",
        "          add_special_tokens=True,\n",
        "          pad_to_max_length='right')\n",
        "        else:\n",
        "            t_e = tokenizer.encode_plus(text=e, \n",
        "                                        max_length=max_len,\n",
        "                                        add_special_tokens=True,\n",
        "                                        pad_to_max_length='right')\n",
        "        input_ids.append(t_e['input_ids'])\n",
        "        attention_masks.append(t_e['attention_mask'])\n",
        "        type_ids.append(t_e['token_type_ids'])\n",
        "\n",
        "    return (input_ids, attention_masks, type_ids)\n",
        "\n",
        "\n",
        "def rmse_A(label, pred):\n",
        "    rmse = np.sqrt(np.mean((np.array(label) - np.array(pred)) ** 2))\n",
        "    return rmse\n",
        "\n",
        "'''\n",
        "def to_csv(path, df, pred):\n",
        "    output = pd.DataFrame({'id':df['id'],  'pred':pred})\n",
        "    output.to_csv(path, index=False)\n",
        "    with zipfile.ZipFile(path[:-4] + '.zip', 'w') as zf:\n",
        "        zf.write(path)\n",
        "'''\n",
        "\n",
        "def make_even(num):\n",
        "    arr = np.linspace(0, 3, 16)\n",
        "    diff, ans = (3, -1)\n",
        "    for e in arr:\n",
        "        if abs(e - num) < diff:\n",
        "            diff = abs(e - num)\n",
        "            ans = e\n",
        "\n",
        "    return round(ans, 1)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    \"\"\"\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    \"\"\"\n",
        "    elapsed_rounded = int(round(elapsed))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "def setup_seed(seed):\n",
        "    CUDA = torch.cuda.is_available()\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if CUDA:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def gpu_info():\n",
        "    if torch.cuda.device_count() > 0:\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            print('cuda {}: {}'.format(i, torch.cuda.get_device_name(i)))\n",
        "           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj34UfOQZJ2z"
      },
      "source": [
        "class My_BERT:\n",
        "\n",
        "    def __init__(self, pair=True, model_name='bert-base-cased', max_len=256):\n",
        "        self.device = torch.device(\n",
        "            'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.CONFIG = {\n",
        "            'A_train_p': '/content/gdrive/MyDrive/data/task1/train.csv', \n",
        "            'A_dev_p': '/content/gdrive/MyDrive/data/task1/dev.csv',\n",
        "            'A_test_p': '/content/gdrive/MyDrive/data/task1/truth_task_1.csv'\n",
        "        }\n",
        "        self.pair = pair\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\n",
        "            model_name, num_labels=1).to(self.device)\n",
        "        self.max_len = max_len\n",
        "    def fineT(self, epochs=1, bs=16, verbose=False):\n",
        "\n",
        "        if verbose:\n",
        "            print('you are now using {}.'.format(\n",
        "            torch.cuda.get_device_name(self.device)))\n",
        "\n",
        "        # read dataset\n",
        "        \n",
        "        a_t_s, a_t_l = Read_data(\n",
        "                self.CONFIG['A_train_p']).reader(pair=self.pair)\n",
        "        a_v_s, a_v_l = Read_data(\n",
        "                self.CONFIG['A_dev_p']).reader( pair=self.pair)\n",
        "        s, l = a_t_s + a_v_s, a_t_l + a_v_l\n",
        "\n",
        "        #print(s)\n",
        "        #print(l)\n",
        "        #tokenization\n",
        "        t_ids, t_masks, t_types = (torch.tensor(x)\n",
        "                           for x in get_pair_ids(s, self.tokenizer, max_len=self.max_len, pair=self.pair))\n",
        "        # create dataloader\n",
        "\n",
        "        train_data = TensorDataset(t_ids, t_masks, t_types, torch.tensor(l))\n",
        "        train_dataloader = DataLoader(train_data, shuffle=True, batch_size=bs)\n",
        "\n",
        "        # optimizer initial\n",
        "        optimizer = AdamW(self.model.parameters(), lr=2e-05, eps=1e-08)\n",
        "        total_steps = len(train_dataloader) * epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "        # start training\n",
        "        loss_values = []\n",
        "        self.model.zero_grad()\n",
        "        for epoch_i in range(epochs):\n",
        "            if verbose:\n",
        "                print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "\n",
        "            t0 = time.time()\n",
        "            total_loss = 0\n",
        "            self.model.train()\n",
        "\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                if step % 100 == 0 and not step == 0:\n",
        "                    elapsed = format_time(time.time() - t0)\n",
        "                    if verbose:\n",
        "                        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(\n",
        "                            step, len(train_dataloader), elapsed))\n",
        "\n",
        "                b_input_ids, b_input_mask, b_types, b_labels = tuple(\n",
        "                    (t.to(self.device) for t in batch))\n",
        "                outputs = self.model(input_ids=b_input_ids, token_type_ids=b_types,\n",
        "                                attention_mask=b_input_mask, labels=b_labels)\n",
        "                loss = outputs[0]\n",
        "                total_loss += loss.item()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                self.model.zero_grad()\n",
        "\n",
        "            avg_train_loss = total_loss / len(train_dataloader)\n",
        "            loss_values.append(avg_train_loss)\n",
        "            if verbose:\n",
        "                print('  Average training loss: {0:.2f}'.format(\n",
        "                    avg_train_loss))\n",
        "                print('  Training epcoh took: {:}'.format(\n",
        "                    format_time(time.time() - t0)))\n",
        "\n",
        "        if verbose:\n",
        "            print('Training Finished.')\n",
        "\n",
        "        grand=self.model.state_dict()\n",
        "        \n",
        "        file_name = '/gdrive/MyDrive/data/pretrainedc/{2}/Bert_{2}_regress_{0}epochs_{1}bs.pt'.format(\n",
        "            epochs, bs, 'pair' if self.pair else 'single')\n",
        "\n",
        "        torch.save(self.model.state_dict(), os.getcwd() + file_name)\n",
        "\n",
        "        print(file_name + ' saved.')\n",
        "        torch.cuda.empty_cache()\n",
        "        return self\n",
        "\n",
        "    def predict(self, pretrained=None, task='A', bs=128):\n",
        "        test_s, test_l = Read_data(\n",
        "            self.CONFIG['A_test_p']).reader( pair=self.pair)\n",
        "\n",
        "\n",
        "        # create dataloader\n",
        " \n",
        "        ids, masks, types = (torch.tensor(x) for x in get_pair_ids(\n",
        "                test_s, self.tokenizer, max_len=self.max_len, pair=self.pair))\n",
        "        \n",
        "        data = TensorDataset(ids, masks, types)\n",
        "        dataloader = DataLoader(data, batch_size=bs)\n",
        "\n",
        "        tag = 'pair' if self.pair else 'single'\n",
        "        \n",
        "        if pretrained:\n",
        "            self.model.load_state_dict(torch.load(\n",
        "                    '{0}/gdrive/MyDrive/data/pretrained/{1}/{2}'.format(os.getcwd(), tag, pretrained)))\n",
        "        self.model.eval()\n",
        "\n",
        "        pred = []\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            b_input_ids, b_input_mask, b_types = tuple(\n",
        "                (t.to(self.device) for t in batch))\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(\n",
        "                    input_ids=b_input_ids, token_type_ids=b_types, attention_mask=b_input_mask)\n",
        "            logits = outputs[0]\n",
        "            #print(len(logits))\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            pred.extend(logits)\n",
        "        \n",
        "        return pred\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, pretrained=None, task='A',  bs=128):\n",
        "        pred = [make_even(x) for x in self.predict(pretrained=pretrained, task=task, bs=bs)]\n",
        "        test_s, test_l = Read_data(\n",
        "            self.CONFIG['A_test_p']).reader( pair=self.pair)\n",
        "        \n",
        "        res =rmse_A(test_l, pred)\n",
        "        print('{} model task A RMSE = {:.5f}'.format(\n",
        "                pretrained[:-3] if pretrained else 'this model', res))\n",
        "        print(res)\n",
        "        return round(res, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3UlhnsHZYbk"
      },
      "source": [
        "def get_res(tag, pair=True, max_len=CONFIG['pair_max_len'], task='A'):\n",
        "    # for pair regress\n",
        "    if not os.path.exists(os.getcwd() +'/gdrive/MyDrive/data/pretrained/'):\n",
        "       print('no pretrained model files, please train first!')\n",
        "    pretrains = os.listdir(os.getcwd()+'/gdrive/MyDrive/data/pretrained/' + tag)\n",
        "    model = My_BERT(pair=pair, max_len=max_len)\n",
        "    res = [model.evaluate(pretrained=pretrains[i], task=task) for i in range(0,len(pretrains))]\n",
        "    del model\n",
        "    return res\n",
        "\n",
        "def get_A():\n",
        "    return {\n",
        "        'pair_regress': get_res('pair', pair=True, max_len=CONFIG['pair_max_len'], task='A'),\n",
        "        #'single_regress': get_res('single', pair=False, max_len=CONFIG['single_max_len'], task='A'),\n",
        "        \n",
        "    }\n",
        "\n",
        "    \n",
        "\n",
        "def save_res():\n",
        "    res = {\n",
        "        'A': get_A(),\n",
        "        \n",
        "    }\n",
        "    with open('results.pickle', 'wb') as f:\n",
        "        pickle.dump(res, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    return res\n",
        "\n",
        "\n",
        "\n",
        "def train_bert(pair=False, model='bert-base-cased', max_len=CONFIG['single_max_len']):\n",
        "    My_BERT(pair=pair, model_name=model,max_len=max_len).fineT(epochs=1,bs=32,verbose=True)\n",
        "\n",
        "def train():\n",
        "    # training part, training file will be saved in pretrained folder\n",
        "    # for bert pair regressor training\n",
        "    train_bert(pair=True, max_len=CONFIG['pair_max_len'])\n",
        "    # for bert single regressor training\n",
        "    #train_bert(pair=False, max_len=CONFIG['single_max_len'])\n",
        "   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J7EP-wiZno8"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSkxGE7EcQpe"
      },
      "source": [
        "save_res()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glSwf5jqA3Sy"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import time\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyHQ3ghtCZGX"
      },
      "source": [
        "import tensorflow as tf \n",
        "import tensorflow_hub as hub \n",
        "import tensorflow.compat.v1 as tf1\n",
        "tf1.disable_eager_execution()\n",
        "tf1.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV-IixshIEEt"
      },
      "source": [
        "CONFIG = {\n",
        "            'A_train_p': '/content/gdrive/MyDrive/data/task1/train.csv',\n",
        "            'A_dev_p': '/content/gdrive/MyDrive/data/task1/dev.csv',\n",
        "            'A_test_p': '/content/gdrive/MyDrive/data/task1/truth_task_1.csv',\n",
        "            'B_train_p': '/content/gdrive/MyDrive/data/task2/train.csv',\n",
        "            'B_dev_p': '/content/gdrive/MyDrive/data/task2/dev.csv',\n",
        "            'B_test_p': '/content/gdrive/MyDrive/data/task2/truth_task_2.csv',\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rstpDOVLZ402"
      },
      "source": [
        "def get_data():\n",
        "        t_s, t_l = Read_data(\n",
        "            CONFIG['A_train_p']).reader(pair=False)\n",
        "        v_s, v_l = Read_data(\n",
        "            CONFIG['A_dev_p']).reader(pair=False)\n",
        "        tv_s, tv_l = t_s + v_s, t_l + v_l\n",
        "\n",
        "        test_s, test_l = Read_data(CONFIG['A_test_p']).reader(\n",
        "             pair=False)\n",
        "\n",
        "       \n",
        "        return (tv_s, tv_l, test_s, test_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzv5HIfpZ5QV"
      },
      "source": [
        "tv_f,tv_l,test_s,test_l=get_data()\n",
        "print(len(tv_f))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_ZfsMyKZ6uG"
      },
      "source": [
        "from gensim.models import FastText\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "corpus= tv_f+test_s\n",
        "docs = [d.lower().split() for d in corpus]\n",
        "#print(docs)\n",
        "# train fasttext from gensim api\n",
        "\n",
        "ft = FastText(size=100, window=2, min_count=1, seed=33)\n",
        "ft.build_vocab(docs)\n",
        "ft.train(docs, total_examples=ft.corpus_count, epochs=10)\n",
        "\n",
        "# prepare text for keras neural network\n",
        "\n",
        "max_len = 96\n",
        "#fitting tokenizer to doc\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True)\n",
        "tokenizer.fit_on_texts(docs)\n",
        "'''\n",
        "sequence_docs = tokenizer.texts_to_sequences(docs)\n",
        "sequence_docs = tf.keras.preprocessing.sequence.pad_sequences(sequence_docs, maxlen=max_len,padding='post')\n",
        "#print(sequence_docs[0])\n",
        "#print(len(sequence_docs[0]))\n",
        "# extract fasttext learned embedding and put them in a numpy array\n",
        "'''\n",
        "embedding_matrix_ft = np.random.random((len(tokenizer.word_index) + 1, ft.vector_size))\n",
        "\n",
        "pas = 0\n",
        "for word,i in tokenizer.word_index.items():\n",
        "    \n",
        "    try:\n",
        "        embedding_matrix_ft[i] = ft.wv[word]\n",
        "    except:\n",
        "        pas+=1\n",
        "print(type(embedding_matrix_ft))\n",
        "print(embedding_matrix_ft.shape)\n",
        "print(ft.vector_size)\n",
        "# define a keras model and load the pretrained fasttext weights matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL8Cm-4IZ8W1"
      },
      "source": [
        "sequence_docs_train = tokenizer.texts_to_sequences(tv_f)\n",
        "sequence_docs_train = tf.keras.preprocessing.sequence.pad_sequences(sequence_docs_train, maxlen=max_len,padding='post')\n",
        "sequence_docs_test = tokenizer.texts_to_sequences(test_s)\n",
        "sequence_docs_test = tf.keras.preprocessing.sequence.pad_sequences(sequence_docs_test, maxlen=max_len,padding='post')\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index) + 1, ft.vector_size, \n",
        "                weights=[embedding_matrix_ft],input_length=max_len, trainable=True))\n",
        "\n",
        "model.add(LSTM(6))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(sequence_docs_train,tv_l,epochs=4,verbose=1,batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fdP1wGAci-8"
      },
      "source": [
        "preds=model.predict(sequence_docs_test)\n",
        "print(len(preds))\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKPPbneJaCLN"
      },
      "source": [
        "print(rmse_A(test_l,preds.tolist()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpS_EUepFje9"
      },
      "source": [
        "class Bert_fasttext_LSTM:\n",
        "    def __init__(self, preds, task='A', best_bert='Bert_pair_regress_1epochs_32bs.pt', bert_weight=0.91, fasttextlstm_weight=0.09):\n",
        "        self.task = task\n",
        "        self.CONFIG = {\n",
        "            'pair_max_len': 160,\n",
        "            'single_max_len': 96,  # max length for train, dev, test dataset is 74\n",
        "            'A_test_p': '/content/gdrive/MyDrive/data/task1/truth_task_1.csv',\n",
        "            'B_test_p': '/content/gdrive/MyDrive/data/task2/truth_task_2.csv',\n",
        "        }\n",
        "        self.best_bert = best_bert\n",
        "        self.pair = True if 'pair' in best_bert else False\n",
        "        self.bert_weight = bert_weight\n",
        "        self.fasttextlstm_weight = fasttextlstm_weight\n",
        "        self.fasttext_lstm_pred= preds\n",
        "\n",
        "    def predict(self):\n",
        "\n",
        "        bert_pred = My_BERT(pair=self.pair,\n",
        "                                 max_len=self.CONFIG['pair_max_len'] ).predict(\n",
        "                                     task=self.task, pretrained=self.best_bert)\n",
        "        \n",
        "        print(len(bert_pred))\n",
        "        print(len(self.fasttext_lstm_pred))\n",
        "        pred = [make_even(self.bert_weight*x + (np.asarray(self.fasttextlstm_weight)*y).tolist())\n",
        "                for (x, y) in zip(bert_pred, self.fasttext_lstm_pred)]\n",
        "        return pred\n",
        "    \n",
        "    def evaluate(self):\n",
        "        pred = self.predict()\n",
        "        test_s, test_l = Read_data(\n",
        "            self.CONFIG['A_test_p']).reader( pair=self.pair)\n",
        "        res = rmse_A(test_l, pred)\n",
        "        \n",
        "        print('{0}*{1} + {2}*fasttext_LSTM model task A RMSE = {3:.5f}'.format(\n",
        "            self.bert_weight, self.best_bert[:-3], self.fasttextlstm_weight, res))\n",
        "        return round(res, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-lrTKgFFj-M"
      },
      "source": [
        "Bert_fasttext_LSTM(preds.tolist(),task='A',\n",
        "             best_bert='Bert_pair_regress_1epochs_32bs.pt').evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWoKced-DyFX"
      },
      "source": [
        "\n",
        "\n",
        "class NB_LSTM:\n",
        "\n",
        "    def __init__(self, task='A'):\n",
        "        self.CONFIG = {\n",
        "            'A_train_p': '/content/gdrive/MyDrive/data/task1/train.csv',\n",
        "            'A_dev_p': '/content/gdrive/MyDrive/data/task1/dev.csv',\n",
        "            'A_test_p': '/content/gdrive/MyDrive/data/task1/truth_task_1.csv',\n",
        "            'B_train_p': '/content/gdrive/MyDrive/data/task2/train.csv',\n",
        "            'B_dev_p': '/content/gdrive/MyDrive/data/task2/dev.csv',\n",
        "            'B_test_p': '/content/gdrive/MyDrive/data/task2/truth_task_2.csv',\n",
        "        }\n",
        "        self.task = task\n",
        "        self.retok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
        "        self.model = Sequential()\n",
        "        \n",
        "        self.tv_f = None\n",
        "        self.tv_l = None\n",
        "        self.test_f = None\n",
        "        self.test_l = None\n",
        "\n",
        "    def get_data(self):\n",
        "        t_s, t_l = Read_data(\n",
        "            self.CONFIG['A_train_p']).reader( pair=False)\n",
        "        v_s, v_l = Read_data(\n",
        "            self.CONFIG['A_dev_p']).reader( pair=False)\n",
        "        tv_s, tv_l = t_s + v_s, t_l + v_l\n",
        "\n",
        "        test_s, test_l = Read_data(self.CONFIG['A_test_p']).reader(\n",
        "             pair=False)\n",
        "\n",
        "        return (tv_s, tv_l, test_s, test_l)\n",
        "\n",
        "    def tokenize(self, s):\n",
        "        return self.retok.sub(r' \\1 ', s).split()\n",
        "\n",
        "    def pr(self, trn_term_doc, y_i, y):\n",
        "        p = trn_term_doc[(y == y_i)].sum(0)\n",
        "        return (p + 1) / ((y == y_i).sum() + 1)\n",
        "\n",
        "    def get_features(self):\n",
        "        tv_s, tv_l, test_s, test_l = self.get_data()\n",
        "        vec = TfidfVectorizer(ngram_range=(1, 2), tokenizer=(self.tokenize), min_df=3,\n",
        "                              max_df=0.9,\n",
        "                              strip_accents='unicode',\n",
        "                              use_idf=1,\n",
        "                              smooth_idf=1,\n",
        "                              sublinear_tf=1)\n",
        "        trn_term_doc = vec.fit_transform(tv_s) #sparse matrix\n",
        "        test_term_doc = vec.transform(test_s) #sparse matrix\n",
        "        print(type(trn_term_doc))\n",
        "        print(type(test_term_doc))\n",
        "      \n",
        "        r = np.log(self.pr(trn_term_doc, 1, np.array(tv_l)) /\n",
        "                   self.pr(trn_term_doc, 0, np.array(tv_l)))\n",
        "        tv_f = trn_term_doc.multiply(r)\n",
        "        test_f = test_term_doc.multiply(r)\n",
        "        \n",
        "        \n",
        "        tv_f=tv_f.toarray()\n",
        "        test_f=test_f.toarray()\n",
        "        \n",
        "        return (tv_f, tv_l, test_f, test_l)\n",
        "\n",
        "    def train(self):\n",
        "        self.tv_f, self.tv_l, self.test_f, self.test_l = self.get_features()\n",
        "        print(self.tv_f.shape)\n",
        "        self.tv_l = np.array(self.tv_l)\n",
        "        self.tv_f = self.tv_f[:, :, None] \n",
        "        print(self.tv_f.shape)\n",
        "        print(self.tv_f.shape[1:])\n",
        "        \n",
        "        self.model.add(LSTM(units=1,input_shape = self.tv_f.shape[1:]))\n",
        "        #self.model.add(LSTM(units=10))\n",
        "        self.model.add(Dense(1))\n",
        "        self.model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "        \n",
        "        self.model.fit(self.tv_f, self.tv_l,epochs=1,verbose=1,batch_size=128)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self):\n",
        "    \n",
        "        #self.test_f = self.test_f[:, :, None]\n",
        "        return self.model.predict(self.test_f)\n",
        "\n",
        "\n",
        "    def evalute(self):\n",
        "        if self.task == 'A':\n",
        "            preds=self.predict()\n",
        "            print(preds.shape)\n",
        "            print(preds)\n",
        "            preds=preds.reshape((preds.shape[0],))    \n",
        "            preds=preds.tolist()\n",
        "            print(preds)\n",
        "            print(self.test_l)\n",
        "            res = rmse_A(self.test_l, self.predict())\n",
        "            print('NB-LSTM model task A RMSE: %.5f' % res)\n",
        "        else:\n",
        "            res = accuracy_score(self.test_l, to_label(self.predict(), len(self.test_l)))\n",
        "            print('NB-LSTM model task B accuracy: %.5f' % res)\n",
        "        return round(res, 5)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3ipa2Q8Fcjd"
      },
      "source": [
        "\n",
        "n=NB_LSTM('A')\n",
        "n.train()\n",
        "n.test_f=n.test_f[:,:,None]\n",
        "preds1=n.model.predict(n.test_f)\n",
        "preds_lstm= preds1.tolist()\n",
        "print(rmse_A(n.test_l,preds_lstm))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}